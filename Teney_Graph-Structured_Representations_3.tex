\documentclass[twocolumn]{article}
\usepackage{indentfirst}
\usepackage{picinpar,graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{flushend,cuted}
\usepackage{float}
\usepackage{flushend,cuted}
\usepackage{multirow}
\usepackage{siunitx}
\setlength{\parindent}{2em}
\author{Chaonan Song}
\title{Graph-Structured Representations for Visual Question Answering}
\begin{document}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\bibliographystyle{plain}
        \maketitle
        \par
        \section{Related work}
         Since Antol \emph{et al.}~\cite{[4]} seminal paper, the task of visual quizzes has received more attention from people. The most recent method is to combine images with deep neural networks. The image is preprocessed by a Convolutional Neural Network (CNN) for image classification, and intermediate features are extracted to describe the image. This problem is usually generated by a Recurrent Neural Network (RNN), such as LSTM, which produces a vector that represents a sequence of words. These two representations are mapped to the joint space through one or more nonlinear layers. Then input them into the classifier of the output vocabulary and predict the final answer. Recent papers on VQA have proposed improvements and changes to this basic idea. See\cite{[25]}.  A great progress of basic method is using attention mechanism ~\cite{[3],[5],[12],[27],[28],[31]}. It models the interaction between the actual content of the input image. Visual input is usually represented as a spatial feature map, not as an overall image width feature. Their method uses a weighted operation, which is the same as their chart representation, and they match it as a sub map. Graphic nodes that represent question words are associated with graphic nodes that represent scene objects. Similarly, Lu\emph{et al.}~\cite{[16]} common concern model determines the attention weights for image regions and problem words. Their best method is to proceed in order. First is the problem-guided visual attention, then the image-guided problem attention.
\begin{table}[H]
            \centering
            \begin{tabular}{|c|c|c|}
            \hline
            % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
            \multicolumn{3}{|c|}{Recall rate at 100 FP on FDDB} \\ \hline
            \tabincell{c}{They describe \\ how to use \\ graph repre\\sentations \\ of scene \\ and question \\ for VQA.} & \tabincell{c}{They showed \\ how to use \\ off-the-shelf language \\ parsing tools \\ to generate \\ graphical representations \\ of grammatical \\ relationships.} & \tabincell{c}{They trained \\ the proposed \\ model on the VQA \\ ¡°abstract scene¡± \\ benchmark \cite{[4]} and \\ improved accuracy\\  and accuracy.}  \\
             \hline
            \end{tabular}
            \caption{Contribution of this article}
            \label{Table1}
        \end{table}
         \par In their case, they found that a single version of the joint version performed better. Their methods have some contribution in Table.~\ref{Table1}. A major contribution of their model is the use of input scenarios and structured representations of the problem. This contrasts with typical CNN and RNN models. The Dynamic Memory Network (DMN) applied to VQA in ~\cite{[26]} also maintains a set representation of the input. Similar with their model, DMN modeling inputs the interaction between different parts. Their method can also take as input the features of any relationship between input parts (edge features in the graph). This specifically allows the use of syntactic dependencies between words after the pre-parsing problem. The neural network in the figure has recently received much attention \cite{[8],[11],[15]}. The method most similar to them is the gating sequence neural network [15], which associates a gating cycle unit (GRU ~\cite{[6])} with each node and updates each node by iteratively passing messages between neighbors. Their formula similarly merges information from neighbors into each node feature through multiple iterations, but using the attention mechanism within the recurring unit does not find any advantage.
\bibliography{Graph-Structured}
\end{document}
