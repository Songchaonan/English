\documentclass[twocolumn]{article}
\usepackage{indentfirst}
\usepackage{picinpar,graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{flushend,cuted}
\usepackage{float}
\usepackage{flushend,cuted}
\usepackage{multirow}
\usepackage{siunitx}
\setlength{\parindent}{2em}
\author{Chaonan Song}
\title{Graph representation of scenes and questions}
\begin{document}
        \maketitle
        \par
        \section{Evaluation}
        \subsection{Evaluation on the "abstract scenes" dataset}
        They report our results on the original "abstract scenes" dataset in Table \ref{Table2}. The evaluation is performed on an automated server that does not allow for an extensive ablative analysis. Anecdotally, performance on the validation set corroborates all findings presented above, in particular the strong benefit of pre-parsing, pretrained word embeddings, and graph processing with a GRU. At the time of their submission,  our method occupies the top place on the leader board in both the open-ended and multiple choice settings. The advantage over existing method is most pronounced on the binary and the counting questions. Refer to Fig. \ref{fig5} and to the supplementary for visualizations of the results. 
        \section{Conclusions}
        They presented a deep neural network for visual question answering that processes graph-structured representations of scenes and questions. This can make use of existing natural language processing tools, in particular pre-trained word embedding and syntactic analysis. The latter shows a significant advantage over the traditional sequential processing of problems \emph{e.g.} with LSTMs. In their opinion, VQA systems are unlikely to learn everything from question/answer examples alone. They believe that any significant improvement in performance will require additional sources of information and supervision. Their explicit processing of the language part is a small step in that direction. It has clearly shown that generalization can be improved without relying entirely on VQA-specific annotations. So far, they have applied their method to the dataset of the clip scene. By replacing the nodes in the input scene graph with suggestions made by pre-trained object detectors, it will solve the direct expansion of the actual image in future work.
        \begin{table*}
            \centering
            \begin{tabular}{l c c c c c c c l}
            Method & Overall & Yes/no & Other & Number & Overall & Yes/no & Other & Number \\ \hline
           LSTM blind \cite{[4]} & 61.41 & 76.90 & 49.19& 49.65 & 57.19 & 76.88 & 38.79 & 49.55 \\
            LSTM with global image features \cite{[4]} & 69.21 & 77.46 & 66.65 & 52.90 & 65.02 & 77.45 & 56.41 & 52.54 \\
            Zhang \emph{et al.} \cite{[30]} (yes/no only) & 35.25 & 79.14 & ¡ª & ¡ª & 35.25& 79.14& ¡ª & ¡ª \\
            Multimodal residual learning \cite{[13]} & 67.99 & 79.08& 61.99& 52.57 & 62.56 & 79.10& 48.90& 51.60 \\
            U. Tokyo MIL (ensemble) \cite{[22],[1]} & 71.18 & 79.59 & 67.93 & 56.19 & 69.73 & 80.70 & \textbf{62.08} & 58.82 \\ \hline
            \textbf{Graph VQA} (full model) & 74.37& 79.74 & 68.31 & 74.97 & 70.42 & 81.26 & 56.28 & 76.47 \\
            \hline
            \end{tabular}
            \caption{Results on the test set of the "abstract scenes" dataset (average scores in percents).}
            \label{Table2}
        \end{table*}
        \begin{figure*}
            \centering
            % Requires \usepackage{graphicx}
            \includegraphics[width=0.8\textwidth]{Graph5.jpg}
            \caption{Qualitative results on the ¡°abstract scenes¡± dataset (top row) and on ¡°balanced¡± pairs (middle and bottom row). We show the input scene, the question, the predicted answer, and the correct answer when the prediction is erroneous. We also visualize the matrices of matching weights (Eq. 6, brighter correspond to higher values) between question words (vertically) and scene objects (horizontally). The matching weights are also visualized over objects in the scene, after summation over words, giving an indication of their estimated relevance. The ground truth object labels are for reference only, and not used for training or inference.}
            \label{fig5}
        \end{figure*}
        
\bibliographystyle{plain}
\newpage
\bibliography{Graph-Structured}
\end{document}